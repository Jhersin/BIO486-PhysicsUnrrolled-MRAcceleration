{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-15T12:17:25.154048Z",
     "start_time": "2025-05-15T12:17:21.810753Z"
    }
   },
   "source": [
    "from Utils2.Dataset import Unrolled_Dataset\n",
    "from Utils2.Unrolled_2iteration import Physics\n",
    "from Utils2.Unrolled_2iteration import SamplingFunction\n",
    "from Utils2.Unrolled_2iteration import UnrolledReconstructor\n",
    "from Utils2.Model import TinyUNET\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn as nn\n",
    "from monai.metrics.regression import SSIMMetric\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1. Dataset and DataLoader",
   "id": "57260fdc8f1a903e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T12:19:37.546236Z",
     "start_time": "2025-05-15T12:17:30.194412Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_path = '/shared/BIOE486/SP25/users/jgarca2/Dataset/multicoil_val_prepro_valid_dataset'\n",
    "dataset = Unrolled_Dataset(unroll_root_dir=data_path, transform=True)\n",
    "train_data = DataLoader(dataset, batch_size=16, shuffle=True, num_workers=16, pin_memory=True)\n",
    "for i, (zf, us, cs, gt) in enumerate(train_data):\n",
    "    print(f\"Batch {i}:\")\n",
    "    print(f\"  ZF shape: {zf.shape}\")  # (B, 1, 320, 320)\n",
    "    print(f\"  US shape: {us.shape}\")  # (B, 15, 640, 115)\n",
    "    print(f\"  CS shape: {cs.shape}\")  # (B, 15, 320, 320)\n",
    "    print(f\"  GT shape: {gt.shape}\")  # (B, 1, 320, 320)\n",
    "    break"
   ],
   "id": "dbb42078d4cf52c2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found files:\n",
      "  Zero-filled:         6661\n",
      "  Coil sensitivity:    6661\n",
      "  Undersampled:        6661\n",
      "  Ground truth (Cs):   6661\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[2]\u001B[39m\u001B[32m, line 4\u001B[39m\n\u001B[32m      2\u001B[39m dataset = Unrolled_Dataset(unroll_root_dir=data_path, transform=\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[32m      3\u001B[39m train_data = DataLoader(dataset, batch_size=\u001B[32m16\u001B[39m, shuffle=\u001B[38;5;28;01mTrue\u001B[39;00m, num_workers=\u001B[32m16\u001B[39m, pin_memory=\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[32m----> \u001B[39m\u001B[32m4\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m i, (zf, us, cs, gt) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(train_data):\n\u001B[32m      5\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mBatch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m:\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m      6\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m  ZF shape: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mzf.shape\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)  \u001B[38;5;66;03m# (B, 1, 320, 320)\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/bioe_dl_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:733\u001B[39m, in \u001B[36m_BaseDataLoaderIter.__next__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    730\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    731\u001B[39m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[32m    732\u001B[39m     \u001B[38;5;28mself\u001B[39m._reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m733\u001B[39m data = \u001B[38;5;28mself\u001B[39m._next_data()\n\u001B[32m    734\u001B[39m \u001B[38;5;28mself\u001B[39m._num_yielded += \u001B[32m1\u001B[39m\n\u001B[32m    735\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[32m    736\u001B[39m     \u001B[38;5;28mself\u001B[39m._dataset_kind == _DatasetKind.Iterable\n\u001B[32m    737\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m._IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    738\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m._num_yielded > \u001B[38;5;28mself\u001B[39m._IterableDataset_len_called\n\u001B[32m    739\u001B[39m ):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/bioe_dl_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1491\u001B[39m, in \u001B[36m_MultiProcessingDataLoaderIter._next_data\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1488\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._process_data(data, worker_id)\n\u001B[32m   1490\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m._shutdown \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m._tasks_outstanding > \u001B[32m0\u001B[39m\n\u001B[32m-> \u001B[39m\u001B[32m1491\u001B[39m idx, data = \u001B[38;5;28mself\u001B[39m._get_data()\n\u001B[32m   1492\u001B[39m \u001B[38;5;28mself\u001B[39m._tasks_outstanding -= \u001B[32m1\u001B[39m\n\u001B[32m   1493\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._dataset_kind == _DatasetKind.Iterable:\n\u001B[32m   1494\u001B[39m     \u001B[38;5;66;03m# Check for _IterableDatasetStopIteration\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/bioe_dl_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1443\u001B[39m, in \u001B[36m_MultiProcessingDataLoaderIter._get_data\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1441\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._pin_memory:\n\u001B[32m   1442\u001B[39m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28mself\u001B[39m._pin_memory_thread.is_alive():\n\u001B[32m-> \u001B[39m\u001B[32m1443\u001B[39m         success, data = \u001B[38;5;28mself\u001B[39m._try_get_data()\n\u001B[32m   1444\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m success:\n\u001B[32m   1445\u001B[39m             \u001B[38;5;28;01mreturn\u001B[39;00m data\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/bioe_dl_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1284\u001B[39m, in \u001B[36m_MultiProcessingDataLoaderIter._try_get_data\u001B[39m\u001B[34m(self, timeout)\u001B[39m\n\u001B[32m   1271\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_try_get_data\u001B[39m(\u001B[38;5;28mself\u001B[39m, timeout=_utils.MP_STATUS_CHECK_INTERVAL):\n\u001B[32m   1272\u001B[39m     \u001B[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001B[39;00m\n\u001B[32m   1273\u001B[39m     \u001B[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m   1281\u001B[39m     \u001B[38;5;66;03m# Returns a 2-tuple:\u001B[39;00m\n\u001B[32m   1282\u001B[39m     \u001B[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001B[39;00m\n\u001B[32m   1283\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1284\u001B[39m         data = \u001B[38;5;28mself\u001B[39m._data_queue.get(timeout=timeout)\n\u001B[32m   1285\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m (\u001B[38;5;28;01mTrue\u001B[39;00m, data)\n\u001B[32m   1286\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m   1287\u001B[39m         \u001B[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001B[39;00m\n\u001B[32m   1288\u001B[39m         \u001B[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001B[39;00m\n\u001B[32m   1289\u001B[39m         \u001B[38;5;66;03m# worker failures.\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/bioe_dl_env/lib/python3.12/queue.py:180\u001B[39m, in \u001B[36mQueue.get\u001B[39m\u001B[34m(self, block, timeout)\u001B[39m\n\u001B[32m    178\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m remaining <= \u001B[32m0.0\u001B[39m:\n\u001B[32m    179\u001B[39m             \u001B[38;5;28;01mraise\u001B[39;00m Empty\n\u001B[32m--> \u001B[39m\u001B[32m180\u001B[39m         \u001B[38;5;28mself\u001B[39m.not_empty.wait(remaining)\n\u001B[32m    181\u001B[39m item = \u001B[38;5;28mself\u001B[39m._get()\n\u001B[32m    182\u001B[39m \u001B[38;5;28mself\u001B[39m.not_full.notify()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/bioe_dl_env/lib/python3.12/threading.py:359\u001B[39m, in \u001B[36mCondition.wait\u001B[39m\u001B[34m(self, timeout)\u001B[39m\n\u001B[32m    357\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    358\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m timeout > \u001B[32m0\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m359\u001B[39m         gotit = waiter.acquire(\u001B[38;5;28;01mTrue\u001B[39;00m, timeout)\n\u001B[32m    360\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    361\u001B[39m         gotit = waiter.acquire(\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2. Physics",
   "id": "8ec00f7785184aab"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "sampler = SamplingFunction()\n",
    "physics = Physics(alpha=0.1, sampler=sampler)\n",
    "W_e = physics._compute_W_e(us, cs)\n",
    "S = physics._compute_S(zf, cs)\n",
    "input = physics._final_sum(S,W_e)\n",
    "print(W_e.shape, S.shape, input.shape)"
   ],
   "id": "809cdfaf731763af",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3. Deep learning model (UNETxComplex)",
   "id": "cd02bd32617adb9f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "log_dir=\"/shared/BIOE486/SP25/users/jgarca2/Experiments/Graph\"\n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "# Data\n",
    "real = torch.randn(1, 1, 320, 320)\n",
    "imag = torch.randn(1, 1, 320, 320)\n",
    "model_input = torch.complex(real, imag)\n",
    "model = TinyUNET()\n",
    "# Log the graph\n",
    "writer.add_graph(model, model_input)\n",
    "writer.close()"
   ],
   "id": "ea6d87483213a460",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4. Unrroll physics and DL model",
   "id": "599b1db29ae37c82"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Set device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Sent the data to device\n",
    "model1, model2 = TinyUNET().to(device), TinyUNET().to(device)\n",
    "zf, us, cs = zf.to(device), us.to(device), cs.to(device)\n",
    "\n",
    "# Initialize the unrolled reconstructor\n",
    "reconstructor = UnrolledReconstructor(model1, model2).to(device)\n",
    "\n",
    "# Forward pass\n",
    "output = reconstructor(zf, us, cs)\n",
    "print(f\"Output shape: {output.shape}\")  # Expected: [32, 1, 320, 320]\n"
   ],
   "id": "4dd09c5fbc9fcef4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 6. Training loop",
   "id": "2d410a896c679406"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class LogMSELoss(nn.Module):\n",
    "    def __init__(self, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        pred_log = torch.log(torch.abs(pred) + self.eps)\n",
    "        target_log = torch.log(torch.abs(target) + self.eps)\n",
    "        return torch.mean((pred_log - target_log) ** 2)\n",
    "\n",
    "# Loss & Metrics\n",
    "criterion = LogMSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "ssim_metric = SSIMMetric(spatial_dims=2)\n",
    "\n",
    "\n",
    "model1, model2 = TinyUNET().to(device), TinyUNET().to(device)\n",
    "reconstructor = UnrolledReconstructor(model1, model2).to(device)\n",
    "\n",
    "for epoch in range(1, 10):\n",
    "    reconstructor.train()\n",
    "    train_loss = 0\n",
    "    ssim_total = 0\n",
    "    ssim_count = 0\n",
    "\n",
    "    for i, (zf, us, cs,  gt) in enumerate(train_data):\n",
    "        print(f\"Epoch {epoch}, Batch {i+1}\", end='\\r')\n",
    "\n",
    "        # Move to device\n",
    "        zf, us, cs, gt = zf.to(device), us.to(device), cs.to(device), gt.to(device)\n",
    "\n",
    "        # --- Normalize inputs to match GT scale ---\n",
    "        gt_max = gt.abs().amax(dim=(-1, -2), keepdim=True)\n",
    "        zf = zf / (gt_max + 1e-8)\n",
    "        cs = cs / (gt_max + 1e-8)\n",
    "\n",
    "        # Forward\n",
    "        pred = reconstructor(zf, us, cs)\n",
    "\n",
    "        # Loss on log-magnitude or raw magnitude\n",
    "        loss = criterion(pred, gt)\n",
    "\n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * zf.size(0)\n",
    "\n",
    "        # --- SSIM ---\n",
    "        # Convert to magnitude\n",
    "        pred_mag = torch.abs(pred)\n",
    "        gt_mag = torch.abs(gt)\n",
    "\n",
    "        # Normalize to [0, 1] (optional but helps SSIM stability)\n",
    "        pred_mag = pred_mag / (pred_mag.max() + 1e-8)\n",
    "        gt_mag = gt_mag / (gt_mag.max() + 1e-8)\n",
    "\n",
    "        ssim_score = ssim_metric(pred_mag, gt_mag)\n",
    "        ssim_total += ssim_score.mean().item()\n",
    "        ssim_count += 1\n",
    "\n",
    "    epoch_train_loss = train_loss / len(train_data.dataset)\n",
    "    epoch_ssim = ssim_total / ssim_count\n",
    "    print(f\"Epoch {epoch:02d} | Loss: {epoch_train_loss:.4f} | SSIM: {epoch_ssim:.4f}\")"
   ],
   "id": "27ad78a101be45af",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 7. Final reconstruction",
   "id": "9a0e4afe9b5ced39"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def show_reconstruction_logscaled(zf, recon, gt, idx=0, eps=1e-5):\n",
    "    \"\"\"\n",
    "    Visualize log-magnitude of zerofill, reconstruction, and ground truth\n",
    "    with shared vmin/vmax and a single colorbar.\n",
    "\n",
    "    Parameters:\n",
    "        zf, recon, gt: complex-valued tensors [B, 1, H, W]\n",
    "        idx: index of the image in the batch to visualize\n",
    "        eps: stability epsilon for log\n",
    "    \"\"\"\n",
    "    # Compute log-magnitude images\n",
    "    zf_mag = np.log(np.abs(zf[idx, 0].cpu().detach().numpy()) + eps)\n",
    "    recon_mag = np.log(np.abs(recon[idx, 0].cpu().detach().numpy()) + eps)\n",
    "    gt_mag = np.log(np.abs(gt[idx, 0].cpu().detach().numpy()) + eps)\n",
    "\n",
    "    # Stack for shared color scaling\n",
    "    all_imgs = np.stack([zf_mag, recon_mag, gt_mag], axis=0)\n",
    "    vmin, vmax = np.min(all_imgs), np.max(all_imgs)\n",
    "\n",
    "    titles = ['Zerofill', 'Reconstruction', 'Ground Truth']\n",
    "    images = [zf_mag, recon_mag, gt_mag]\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(10, 10), constrained_layout=True)\n",
    "\n",
    "    for ax, img, title in zip(axes, images, titles):\n",
    "        im = ax.imshow(img, cmap='gray', vmin=vmin, vmax=vmax)\n",
    "        ax.set_title(title, fontsize=14)\n",
    "        ax.axis('off')\n",
    "\n",
    "    # Shared colorbar\n",
    "    fig.colorbar(im, ax=axes, fraction=0.015, pad=0.04)\n",
    "    plt.show()"
   ],
   "id": "ad540e2a013d7c5b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for i, (zf, us, cs, gt) in enumerate(train_data):\n",
    "    print(f\"Batch {i}:\")\n",
    "    print(f\"  ZF shape: {zf.shape}\")  # (B, 1, 320, 320)\n",
    "    print(f\"  US shape: {us.shape}\")  # (B, 15, 640, 115)\n",
    "    print(f\"  CS shape: {cs.shape}\")  # (B, 15, 320, 320)\n",
    "    print(f\"  GT shape: {gt.shape}\")  # (B, 1, 320, 320)\n",
    "    break\n",
    "\n",
    "zf, us, cs, gt = zf.to(device), us.to(device), cs.to(device), gt.to(device)\n",
    "output = reconstructor(zf, us, cs)"
   ],
   "id": "ebc6abc394ece006",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "show_reconstruction_logscaled(zf, output, gt, idx = 31, eps=1e-4)",
   "id": "25ef3dc34baa8b54",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
